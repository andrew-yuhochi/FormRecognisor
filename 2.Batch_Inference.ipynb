{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1ece8c-d2b3-4a22-beec-563201f281a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec7c8c0-2bc6-4859-9d57-70e3d90dc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.applications import resnet\n",
    "import imageio as io\n",
    "import pandas as pd\n",
    "from keras.models import Model, load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cef6288-5b6b-44c0-9e64-e3cc5d4f26c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10600 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# set up the base model and load the weight\n",
    "sample_img = data_path + \"/test/test/10041_1.jpeg\" \n",
    "input_shape = io.imread(sample_img).shape\n",
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=resnet.preprocess_input)\n",
    "    \n",
    "test_dir = data_path + \"/test\"\n",
    "    \n",
    "# generator function for data augmentation\n",
    "def multi_input_data_gen(X1_dir, X2_dir, X3_dir, batch_size, seed):\n",
    "            \n",
    "    genX1 = datagen.flow_from_directory(X1_dir, (input_shape[0], input_shape[1]), \n",
    "                                                batch_size=batch_size, seed=seed, shuffle=False)\n",
    "            \n",
    "    genX2 = datagen.flow_from_directory(X2_dir, (input_shape[0], input_shape[1]), \n",
    "                                                batch_size=batch_size, seed=seed, shuffle=False)\n",
    "            \n",
    "    genX3 = datagen.flow_from_directory(X3_dir, (input_shape[0], input_shape[1]), \n",
    "                                                batch_size=batch_size, seed=seed, shuffle=False)\n",
    "    \n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        X2i = genX2.next()\n",
    "        X3i = genX3.next()\n",
    "\n",
    "        yield [X1i[0], X2i[0], X3i[0]]\n",
    "                \n",
    "\n",
    "test_data_gen = datagen.flow_from_directory(directory =  data_path + \"/test\", \n",
    "                                                    target_size = (input_shape[0], input_shape[1]), \n",
    "                                                    batch_size = 1, \n",
    "                                                    class_mode = None, \n",
    "                                                    shuffle = False, seed=seed)\n",
    "def number_of_data(data_path, dataset):\n",
    "    \n",
    "    if dataset != \"test\":\n",
    "        dataset_path = data_path + \"/\" + dataset + \"/anchor\"\n",
    "    else:\n",
    "        dataset_path = data_path + \"/\" + dataset\n",
    "    arr1 = os.listdir(dataset_path)\n",
    "    arr1.sort()\n",
    "    \n",
    "    img_number = 0\n",
    "    if arr1[0] == '.ipynb_checkpoints':\n",
    "        arr1 = arr1[1:]\n",
    "    \n",
    "    for i in range(len(arr1)):\n",
    "        img_path = dataset_path + \"/\" + arr1[i]\n",
    "        arr2 = os.listdir(img_path)\n",
    "        arr2.sort()\n",
    "        arr2\n",
    "        if arr2[0] == '.ipynb_checkpoints':\n",
    "            arr2 = arr2[1:]\n",
    "            \n",
    "        if dataset == \"test\":\n",
    "            classes = []\n",
    "            for i in arr2:\n",
    "                classes.append(i[0:5])\n",
    "            class_num = len(set(classes))\n",
    "        else:\n",
    "            class_num = len(arr1)            \n",
    "                \n",
    "        img_number = img_number + len(arr2)\n",
    "    return class_num, img_number\n",
    "\n",
    "_ , test_step_size = number_of_data(data_path, \"test\") \n",
    "test_step_size = test_step_size//1\n",
    "\n",
    "test_data_gen.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4255251-8bc6-43c0-a6d2-eea7c94da2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "10600/10600 [==============================] - 232s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "base_model = load_model(model_save_path + \"/\" + names + \".h5\")\n",
    "\n",
    "embedd_test_data = pd.DataFrame(base_model.predict(test_data_gen, steps=test_step_size, verbose=1), index = test_data_gen.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af6beab-c72b-47dc-bae7-42116a6a1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass through the reference data to the model and write their embedding into a file\n",
    "img_list = os.listdir(reference_data_path)\n",
    "img_list = set([img[0:5] for img in img_list if img[0] != \".\"])\n",
    "    \n",
    "def image_preprocess(path):\n",
    "    img = load_img(path, grayscale=False, color_mode=\"rgb\", target_size=(1024, 768), interpolation=\"nearest\")\n",
    "    return (img_to_array(img)).reshape((1,1024, 768,3))\n",
    "\n",
    "emb = [base_model(resnet.preprocess_input(image_preprocess(reference_data_path + i + \".jpg\"))).numpy().reshape((256)) for i in img_list]\n",
    "    \n",
    "embedding = pd.DataFrame(emb, index = img_list).T\n",
    "embedding.to_csv(reference_embedding_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ceee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity between each testing data & each reference embedding\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "def cal_cos_sim(embedd_test_data, embedd_database):\n",
    "    \n",
    "    img_dot_database = (embedd_test_data @ embedd_database)\n",
    "    img_len = np.sqrt((embedd_test_data * embedd_test_data).sum(axis = 1))\n",
    "    database_len = np.sqrt((embedd_database * embedd_database).sum())\n",
    "    \n",
    "    cos_sim = (img_dot_database/(pd.DataFrame(img_len) @ pd.DataFrame(database_len).T))\n",
    "    colums_to_be_kept = np.append([\"Classes\", \"Classes_index\"], cos_sim.columns.values)\n",
    "    cos_sim[\"ori_idex\"] = cos_sim.index\n",
    "    cos_sim[[\"Dataset\", \"Filename\"]] = cos_sim[\"ori_idex\"].str.split('\\\\', 1, expand=True)\n",
    "    cos_sim[[\"Classes\", \"Form_Number\"]] = cos_sim['Filename'].str.split('_', 1, expand=True)\n",
    "    cos_sim.index = cos_sim.Filename\n",
    "    cos_sim[\"Classes_index\"] = cos_sim.Classes.apply(lambda x: cos_sim.columns.values.tolist().index(x))\n",
    "    \n",
    "    return cos_sim[colums_to_be_kept]\n",
    "    \n",
    "# Cal cosine similarity\n",
    "cos_sim = cal_cos_sim(embedd_test_data, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6d0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_path = result_save_path + \"/\" + model_name + \".csv\"\n",
    "#cos_sim.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be8f67c-8b91-4413-ad33-19adf61b9e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 Accuracy :  0.394, Top3 Accuracy:  0.643, Top5 Accuracy:  0.778.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the top K accuracy based on the cosine simiarity\n",
    "Top1Acc = top_k_accuracy_score(cos_sim.Classes_index.values, cos_sim.iloc[:,2:].values, k=1)\n",
    "Top3Acc = top_k_accuracy_score(cos_sim.Classes_index.values, cos_sim.iloc[:,2:].values, k=3)\n",
    "Top5Acc = top_k_accuracy_score(cos_sim.Classes_index.values, cos_sim.iloc[:,2:].values, k=5)\n",
    "    \n",
    "print(\"Top1 Accuracy : %6.3f, Top3 Accuracy: %6.3f, Top5 Accuracy: %6.3f.\" % (Top1Acc, Top3Acc, Top5Acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8799d58-e81a-45f7-912b-e4ccdfce0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top1 Accuracy :  0.334, Top3 Accuracy:  0.565, Top5 Accuracy:  0.711. (0.5)\n",
    "\n",
    "Top1 Accuracy :  0.392, Top3 Accuracy:  0.642, Top5 Accuracy:  0.765. (1)\n",
    "            \n",
    "Top1 Accuracy :  0.394, Top3 Accuracy:  0.643, Top5 Accuracy:  0.778. (2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
